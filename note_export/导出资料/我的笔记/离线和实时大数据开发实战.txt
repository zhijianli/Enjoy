# 前言
> 对于Hive要精深掌握，包括其开发技巧和优化技巧等。MapReduce要掌握执行原理和过程，而MapRedue和HDFS具体的读数据流程、写数据流程、错误处理、调度处理、I/O操作、各种API、管理运维等，站在数据开发的角度，这些都不是必须掌握的。


### 1.1 数据流程
> 在大数据时代，数据量的大当然重要，但更重要的是数据的关联，不同来源数据的结合往往能产生1+1＞2甚至1+1＞10的效果。

> 数据产生的来源有很多，但是其物理存在表现形式主要为文本文件和数据库两种。

> 大数据项目和数据平台构建的实践表明，数据存储处理通常占用整个项目至少1/3以上的时间，这一环节的设计是否合理、最终的数据是否稳定可靠、建模是否灵活可扩展直接决定后续数据应用的成败，也决定了整个数据平台项目的成败。

> 我们可以把数据存储处理工具和技术分为离线处理、近线处理和实时处理，处理后的数据也相应地存储于离线数据仓库、近线数据存储区和实时数据存储区。

> 离线数据仓库中的数据通常是按照某种建模思想（最常用的是维度建模思想）精心组织的，这样既可以使下游用户非常直观和方便地使用，又可以使数据处理过程很方便地扩展和修改。

> SQL抽象层使得实时开发用户不必写Java或者其他编程语言来开发实时处理逻辑，不但大大加快了实时开发的效率，而且大大降低了实时开发的门槛。通常，实时处理的结果会写入实时存储区（比如HBase），以提供高可靠和高并发的实时数据服务，比如用户的实时画像和实时搜索请求，后续的个性化推荐系统或者智能处理程序直接访问此实时存储区就可以实现实时数据服务。

> 数据的精心埋点、海量离线数据同步和毫秒级的实时数据采集、繁琐的数据处理和精心的数据建模，这些都为数据使用奠定了坚实的基础，但是数据最终发挥价值依赖于数据应用环节。

> 数据应用最广泛的方式是“看”，比如决策层和管理人员定时查看的公司/部门业务日报、业务周报和业务月报，一线运营人员查看的运营指标和报表，分析师给业务决策和业务运营参考的数据分析报告，还有分析人员和业务人员不定时的即席分析等。这些数据报表帮助企业管理人员、产品和项目管理人员及一线运营人员定位企业、产品和项目中的问题、隐患和发力方向，并及早采取措施修正方向或者看到正确趋势后加大投入。


### 1.2 数据技术
> 通常来说Flume采集数据的速度和下游处理的速度通常不同步，因此实时平台架构都会用一个消息中间件来缓冲，而这方面最为流行和应用最为广泛的无疑是Kafka。

> Hive目前仍然是包括国际大厂（如Facebook和国内BAT）在内的互联网公司所使用的主流离线数据处理工具，详细内容参见第4章和第5章。

> 由于Spark具有可伸缩、基于内存计算等特点

> HBase是一种构建在HDFS之上的分布式、面向列族的存储系统。在需要实时读写并随机访问超大规模数据集等场景下，HBase目前是市场上主流的技术选择。

> 无模式：每一行都有一个可以排序的主键和任意多的列。列可以根据需要动态增加，同一张表中不同的行可以有截然不同的列。

> 数据多版本：每个单元中的数据可以有多个版本，默认情况下，版本号自动分配，它是单元格插入时的时间戳。

> 数据类型单一：HBase中的数据都是字符串，没有类型。

> 成功。Drill兼容ANSI SQL语法作为接口，支持对本地文件、HDFS、Hive、HBase、MongeDB作为存储的数据查询，文件格式支持Parquet、CSV、TSV以及JSON这种无模式（schema-free）的数据。所有这些数据都可以像使用传统数据库的表查询一样进行快速实时查询。


### 1.3 数据相关从业者和角色
> 大数据时代，数据已经变为生产资料，但是数据真正从生产资料变成生产力变现必须借助专业数据人员的帮助。

> 数据开发工程师也通常是数据咨询的集中点，数据是否能够拿得到？数据在哪里？数据口径如何？数据质量如何？

> 并不是每个算法工程师都要发明算法，但他们需要熟悉常见的各种算法并了解其适用场合，需要查阅文献和论文，时刻关注业界进展，并将它们应用在业务实践中。


### 2.1 离线数据平台的架构、技术和设计
> [插图]
图2-1 离线数据平台的整体架构大图
离线数据平台通常和Hadoop、Hive、数据仓库、ETL、维度建模、数据公共层等联系在一起。
在大数据以及Hadoop没有出现之前，离线数据平台就是数据仓库，数据部门也就是数据仓库部门。即使在今天，在很多对数据相关概念和技术没有较多了解的人看来，数据部门还是数据仓库部门。
Hadoop出现之前，数据仓库的主要处理技术是商业化数据库，比如微软的SQL Server、甲骨文的Oracle、IBM的DB2等。

> 离线数据平台的另一个关键技术是数据的建模，目前采用最为广泛也最为大家认同的是维度建模技术。此外，离线数据内容建设出于最佳实践，通常还会对精心加工后的数据进行分层（ODS原始数据层、DWD明细数据层、DWS汇总层、ADS集市数据层等），本章也将对此进行介绍。

> 而这个和数据分析的需求是天然相反的，数据分析通常需要访问大量的数据，单条数据的分析没有任何意义。数据分析不仅需要访问大量的数据，还要对其进行频繁的统计和查询

> 解决了对生产库的影响问题后，OLTP数据库管理员很快发现备库和复制库还是不能满足数据分析人员的需求，尤其是在性能方面。大量的数据访问通常需要全表扫描，频繁而且通常又是并发地全表扫描会造成OLTP数据库响应异常缓慢甚至宕机，必须有新的理论支撑和技术突破才能够满足这些分析请求。

> 于是OLAP数据库应运而生，它是专门的分析型数据库，是为了满足分析人员的统计分析需求而发展起来的。

> OLTP数据库的分布式（如分库分表等技术）主要是为了解决海量单条数据请求的压力，其主要目的是把所有用户请求均匀分布到每个节点上，而OLAP的分布式是将用户单次对大数据集的请求任务分配到各个节点上独立计算然后再进行汇总返回给用户。

> 所谓列式存储就是将表的每列一列一列地存储在一起，而不是像行存储一样按行把所有字段存储在一起。对于数据库表来说，列的类型是固定的，所以列存储可以很容易采用高压缩比的算法进行压缩和解压缩，磁盘的I/O会大大减少。

> 数据仓库是“在企业管理和决策中面向主题的、集成的、与时间相关的、不可修改的数据集合”

> 与其他数据库应用不同的是，数据仓库更像一种过程，对分布在企业内部各处的业务数据的整合、加工和分析的过程，而不是一种可以购买的产品，这就是他所说的“企业信息化工厂”。

> Inmon认为企业数据仓库应为原子数据的集成仓库，应该用第三范式和ER理论而非维度建模的事实表和维度表来建模。

> Inmon的企业信息化工厂涉及了“数据集市”的概念，所谓“集市”，就是部门级的数据仓库。


### 2.2 实时数据平台的架构、技术和设计
> 大数据和人工智能是天然的一对最佳搭档，尤其是在实时数据方面。

> 对于很多场景来说，实时数据训练的算法效果和离线数据训练的算法效果有着天壤之别，实时数据训练得到的算法用到的数据就是算法正式上线后输入的数据，因此准确性有保障，是算法工程师和业务的首选。

> 流计算的另一个趋势是开发语言不停向声明式语言尤其是流计算SQL的发展。


### 2.3 数据管理
> 专业的数据质量工具和系统能够监测数据行数波动、主键重复、字段空值比、空值率、枚举值检查、枚举值占比、字段最小值、最大值、均值、数值合计等，通过这些指标再叠加业务规则监测来衡量是否有数据质量问题

> 企业的管理层们也许不会关心数据的集成、数据的探查、数据的建模等，甚至有时候对数据质量也有一定的容忍度，但是他们对数据泄露、数据安全出了问题肯定不能容忍，因此数据屏蔽必须慎重考虑和设计。


## 第3章 Hadoop原理实践
> 对于技术人员来说，技术的深度和广度如何把握、深入何种程度以及涉猎到何种范围，是很有意思的问题。

> 笔者认为最重要的是要找到锚点，同时结合工作中涉及的内容和频次以及个人对未来的技术发展规划，就比较容易确定。比如，锚点是数据开发，那么离线数据处理的主要工具Hive是必须极其熟练地掌握和精通的，但Hive背后是Hadoop的HDFS和MapReduce，需要会MapReduce编程么？从笔者的工作实践以及了解来看，这不是必须掌握的，但是数据开发人员必须掌握其概念、架构和工作原理，也就是说，不但要知其然，而且要知其所以然。


### 3.1 开启大数据时代的Hadoop
> Doug用自己儿子的黄色大象玩具的名字“Hadoop”来为此项目命名。

> 业务应用也从单一的搜索扩展到数据处理、分析和挖掘等。

> 但是正如本章开头所说，最重要的是找到锚点。因此以数据为锚点，结合第1章的数据流程图，当前Hadoop生态系统的主要开源技术和框架如图3-1所示。


### 3.2 HDFS和MapReduce优缺点分析
> HDFS是基于流式数据模式访问和处理超大文件的需求而开发的

> 当然，有可能多个副本都会出现问题，但是HDFS保存的时候会自动跨节点和跨机架，因此此种概率非常低，HDFS同时也提供了各种副本放置策略来满足不同级别的容错需求。

> 在多数情况下，分析任务都会涉及数据集的大部分数据，也就是说，对HDFS来说，请求读取整个数据集比请求读取单条记录会更加高效。

> 对于那些有低延时要求的应用程序，HBase是一个更好的选择，尤其适用于对海量数据集进行访问并要求毫秒级响应时间的情况，但HBase的设计是对单行或者少量数据集的访问，对HBase的访问必须提供主键或者主键范围。

> 把需要做什么（What need to do）与具体怎么做（How to do）分开了


### 3.3 HDFS和MapReduce基本架构
> HDFS和MapReduce是Hadoop的两大核心，它们的分工也非常明确，HDFS负责分布式存储，而MapReduce负责分布式计算。

> 从上面的描述可以看出，HDFS和MapReduce共同组成了HDFS体系结构的核心。HDFS在集群上实现了分布式文件系统，MapReduce在集群上实现了分布式计算和任务处理。HDFS在MapReduce任务处理过程中提供了对文件操作和存储等的支持，而Map-Reduce在HDFS基础上实现了任务的分发、跟踪和执行等工作，并收集结果，两者相互作用，完成了Hadoop分布式集群的主要任务。


### 3.4 MapReduce内部原理实践
> Combiner阶段是可选的。Combiner其实也是一种Reduce操作，但它是一个本地化的Reduce操作，是Map运算的本地后续操作


### 3.5 本章小结
> HDFS和MapReduce的知识还牵涉到很多，比如读数据的流程、写数据的流程、错误处理、调度处理、I/O操作、各种API、管理运维等，但是站在数据开发的角度，这些都不是必须掌握的，但MapReduce的工作原理则是必须理解和掌握的，所以本章花了较多内容在这一方面，这也是本章的核心内容，请读者务必掌握。


### 4.1 离线大数据处理的主要技术：Hive
> 因此即使Hive处理的数据集非常小（比如几MB、几十MB），在执行时也会出现延迟现象。这样Hive的性能就不可能很好地和传统的Oracle数据库、MySQL数据库进行比较。Hive不能提供数据排序和查询缓存功能，也不提供在线事务处理，更不提供实时的查询和记录级的更新，但它能更好地处理不变的大规模数据集，当然这是和其根植于Hadoop近似线性的可扩展性分不开的。


### 4.2 Hive SQL
> join时大表放在最后。这是因为每次Map/Reduce任务的逻辑是这样的：Reduce会缓存join序列中除最后一个表之外的所有表的记录，再通过最后一个表将结果序列化到文件系统，因此实践中，应该把最大的那个表写在最后。


### 6.1 大数据建模的主要技术：维度建模
> 度量和环境这两个概念构成了维度建模的基础。而所有维度建模也正是通过对度量和及其上下文和环境的详细设计来实现的。

> 维度建模认为事实表应该包含最底层的、最原子性的细节，因为这样会带来最大的灵活性。

> 这里需要值得注意的是，在进行事实表的设计时，一定要注意一个事实表只能有一个粒度，不能将不同粒度的事实建立在同一张事实表中。

> 维度表包含了事实表所记录的业务过程度量的上下文和环境，它们除了记录“5个W”等信息外，通常还包含了很多的描述字段和标签字段等。

> 维度属性在数据仓库中承担着一个重要的角色。由于它们实际上是所有令人感兴趣的约束条件与报表标签的来源，因此是数据仓库易学易用的关键。

> 在许多方面，数据仓库不过是维度属性的体现而已。数据仓库的能力直接与维度属性的质量和深度成正比。

> 在提供详细的业务用语属性方面所花的时间越多，数据仓库就越好；在属性列值的给定方面所花的时间越多，数据仓库就越好；在保证属性列值的质量方面所花的时间越多，数据仓库就越好。

> 维度表是进入事实表的入口。丰富的维度属性给出了丰富的分析切割能力。维度给用户提供了使用数据仓库的接口。

> 通常可以这样来做出决定，即看字段是一个含有许多取值并参与运算的度量值（当事实看待），还是一个变化不多并作为约束条件的离散取值的描述（当维度属性看待）。

> 星形架构牺牲了部分存储的冗余，但是带来了使用上的极度便捷，也使下游用户的使用和学习成本变得非常低。

> 业务是多变的，模型的设计必须能够经受住业务多变的需求。

> 在实际设计中，可以通过添加新维度或者向维度表中加入维度属性来满足业务新视角的分析需求。

> 大多数情况下，数据仓库模型设计中都会采用星形架构，但是在某些特殊情况下，比如必须使用桥接表的情况下（后文会介绍桥接表）等，必须使用雪花架构。

> 维度建模一般采用具有顺序的4个步骤来进行设计，即选择业务过程、定义粒度、确定维度和确定事实。维度建模的这4个步骤贯穿了维度建模的整个过程和环节，下面逐一介绍。

> 因此，确保数据一致性的最佳办法是从企业和公司全局与整体角度，对于某一个业务过程建立单一的、一致的维度模型。


### 6.2 维度表设计
> 所谓退化维度（degenerate dimension），是指在事实表中那些看起来像是事实表的一个维度关键字，但实际上并没有对应的维度表的字段。退化维度一般都是事务的编号，如购物小票编号、发票编号等。退化维度在分析中通常用来对事实表行进行分组，比如通过购物小票编号可以把顾客某次的购买行为全部关联起来，同时分析人员也可以统计用户的购买频次等。


## 第7章 Hadoop数据仓库开发实战
> 数据相关从业人员则负责将数据和业务结合变现。


