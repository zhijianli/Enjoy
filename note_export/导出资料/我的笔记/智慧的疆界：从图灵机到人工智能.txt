# 前言
> 作为一位前沿的研究人员，读论文以及直接与人交流是更有效率的选择，囿于书籍出版的时滞性，读书更适合作为系统学习的途径而不是了解最前沿成果的途径。


### 1.3 图灵机，计算的基石
> 只要一个问题是可判定的，它的计算过程可以被符号和算法所表达出来，它就可以使用图灵机来完成计算。


### 1.5 图灵测试：何谓智能？
> 因此，图灵在《计算机器和智能》的开篇就直接说明了他不会正面地明确定义何谓“智能”，而是先假定智能可以被机器所模拟，然后对机器是否拥有智能给出了一个侧面的判定途径：

> “如果人类由于无法分辨一台机器是否具备与人类相似的智能，导致无法分辨与之对话的到底是人类还是机器，那即可认定机器存在智能。”

> 迄今为止，没有任何机器能够通过真正意义上的图灵测试。有趣的是，因为机器在图灵测试上一次又一次的失败，人类基于机器通过这种测试的困难度，反而创造出图灵测试最广泛的应用场景，这种应用在网络上随处可见—图形验证码。验证码的英文单词“Captchac”其实就是“通过图灵测试来完全自动地分辨出计算机和人类”这句话的首字母缩写（Completely Automated Public Turing test to tell Computers and Humans Apart）


### 1.6 智能与人类的界限
> 今天主流的人工智能研究中，直接以创造出能通过图灵测试的人造智能机械为目标的研究项目其实极为罕见，这一方面是因为图灵测试的难度实在太高，难以出成果；另一方面是因为图灵测试所提出的目标，其实已经超过了人类自身的需要，好比人类要不断制造更好飞机的目的肯定是为了更快、更舒适地旅行，而不是为了让它飞得更像鸽子以至于能欺骗其他鸟类。


### 1.7 机器能思考吗？
> 哥德尔不完备定理对机械智能的限制是：它决定了无论人类造出多么精致、复杂的机器，只要它还是机器，就将对应一个形式系统￼，就能找到一个在该系统内不可证的公式而使之受到哥德尔不完备定理的打击，机器不能把这个公式作为定理推导出来，但是人心却能看出公式是真的。因此这台机器不可能是承载思维的一个恰当模型￼。

> 对意识理解的局限性，是今天很多人理解人工智能的一个思想误区，总觉得机器必须实现和人类那样自我意识的思考才是真正的人工智能。


### 2.3 达特茅斯会议
> 说明麦卡锡也是充分意识到申请科研基金时拉大旗做虎皮的必要性。


### 2.4 有学术就有江湖
> 人工智能的每一次发展都是试错，每一步突破都伴随着争论和争议而来。


### 2.5 有江湖就有传奇
> 一些从前贝尔的同事认为香农在去麻省理工学院的时候已经精疲力竭，并且对自己开创的信息科学这个领域感到厌倦。可事实上，香农是去做自己真正喜欢的事情去了，什么密码学、信息论、人工智能都只是他的业余爱好，他真正付出了毕生的心血、一直在为之努力的事业就只有—杂耍！读者不要联想太多，“杂耍”这个词并没有别的含义，也不是暗喻，就是指马戏团里那种杂耍。
香农一手创建了信息论，是信息科学的奠基人之一。他一生获奖无数，但在家中所有的奖状和证书都锁在抽屉里，家里最显眼的地方，只放着一张证书—“杂耍学博士”（Doctor of Juggling）￼。读者可能需要一些现代点的类比来帮助消化香农这种网上写成段子都没人会相信的人生：假如你有幸认识一位中科院的院士，或者是某位拿过诺贝尔奖的大拿，你去他家里做客拜访，他热情招待你，但给你展示的不是什么科学理论或者荣誉证书，而是他获得的“星际争霸”游戏（一个暴雪娱乐出品的电子竞技游戏）的世界冠军奖杯。无论你知道这个游戏需要多高的智慧和技巧，肯定都无法抵消你面对这件事情的违和感，对吧。

> 在晚宴上，香农应邀向大家致辞。他说了几分钟，看着乌压压的人群，害怕听众会感到无聊，居然从口袋里掏出三个手抛球开始表演杂耍……观众欢呼了起来，排着队要求签名。研讨会的主席，加州理工学院电气工程学教授罗伯特·麦克利斯（Robert McEliece）回忆那个画面：“那情形……简直就像是牛顿他老人家忽然出现在现代物理学会议上。￼”


### 3.1 概述
> 要去了解一门主义或一个学派，首先要抓住它的核心观点和主要理论，如常识编程、物理符号系统假说，以及他们研究的手段，如推理归纳方法、知识表示方法，还有它取得的主要成果，如问题求解程序、专家系统等。

> 为了实现智能，符号主义学派利用“符号”（Symbolic）来抽象表示现实世界，利用逻辑推理和搜索来替代人类大脑的思考、认知过程，而不去关注现实中大脑的神经网络结构，也不关注大脑是不是通过逻辑运算来完成思考和认知的。


### 3.2 引言：五分钟逻辑学
> 这时的学者不仅仅通过数学工具来研究逻辑，还开始使用逻辑工具来研究数学，逻辑学现在已反过来成为整个现代数学的根基。

> 认知即计算。


### 3.3 描述已知，推理未知
> 逻辑理论家的基本思路是利用计算机的运算速度优势，遍历出形式化之后的定理前提所有可能的变换形式，说白了，其实也就是常见的树搜索，再配以适当的剪支算法。

> 人通常会把要解决的问题分析成一系列子问题，并寻找解决这些子问题的手段，通过解决这些子问题，就能逐渐达成问题的最终解决，GPS解决问题的方法就是模拟这个问题分解的过程，逐步分解问题空间，通过拆分子问题构建搜索树，直到到达已知条件结束搜索。

> 与“认知派”相对，符号主义学派中另外一位主要人物麦卡锡则认为机器不需要模拟人类的思想，而应尝试直接找出抽象推理和解决问题的本质，只要通过逻辑推理能展现出智能行为即可，大可不必去管人类是否使用同样方式思考。所以，由他开创的斯坦福大学一系主要致力于寻找形式化描述客观世界的方法，通过逻辑推理去解决人工智能的问题，因此这一系在符号主义学派中又被称为“逻辑派”。

> 稍微复杂一点的应用，逻辑派的程序就陷入“理论可行，实际不行”的窘境，毫无实用性可言。


### 3.4 知识！知识！知识！
> 机器这样依赖“死记硬背”来获取知识的方法，在应对开放性的问题时就显得捉襟见肘了，甚至在很多问题上都是完全无能为力的。

> 人类智能行为表现出的问题求解能力更多是来源于人所具有的知识，而不仅仅是大脑思考和推理的能力。

> 人脑是如何存储知识的目前尚且不得而知，不过谓词逻辑、语义网络和其他各有特点的知识表示方法合在一起，总算是初步解决了人类如何将知识描述清楚，并存储于计算机的问题，这是一项基础性的工作，后续发展起来的知识工程、知识系统都要建立在这个前提之上。

> 基于知识的系统一般软件架构上都可以划分为知识库和推理引擎两个部分，知识库用于在计算机系统里面反映出真实世界中的知识，而推理引擎则负责使用知识库中的内容推理求解，得到用户提出的问题的答案。

> 典型的基于知识的系统包括决策支持系统、推荐系统、专家系统等，这些系统目前仍然应用非常广泛，其中又以专家系统的历史影响最为突出。


### 3.5 从演绎到归纳
> 说白了，就是让计算机通过若干训练数据，自动构建出一棵分类规则最合理的决策树。


### 4.3 大脑模型
> 皮茨不仅明白这个大脑思维模型所具有的重大意义，还知道应该运用哪些数学工具和技巧去构建这个思维模型，这点正是之前一直困扰没有高深数学基础的麦卡诺克的重大难题。对皮茨一见如故的麦卡洛克邀请这位青年从芝加哥大学的公寓搬到他在芝加哥郊区欣斯代尔（Hinsdale）的家里，与他和家人一同生活。

> 他们提出了一种大脑将信息抽象化，并基于逻辑来处理信息的设想。这种设想尝试解释了人类大脑是如何创造出回荡在脑海里的丰富而精巧的具有层级化的信息的，这个创造过程就被他们称作“思考”。


### 4.4 崛起的明星
> 皮茨在麻省理工学院研究之余，由于维纳的声望和关系，不少著名科学家都开始与皮茨有所交流，这些交流和聚会让双方都获得了很大的帮助和启发，其中最著名的便是冯·诺依曼。维纳与冯·诺依曼一起组织了一个“控制论学家”的学术交流圈子，大家定期聚会探讨心得。


### 4.5 陨落的流星
> 大自然选择了生命的杂乱而非逻辑的严谨，这可能是事实，但也是皮茨难以接受的。


### 4.6 感知机
> 他认为如果两个神经元细胞总是同时被激活的话，它们之间就会出现某种关联，同时激活的概率越高，这种关联程度也会越高。

> 如果两个神经元同时激发，则它们之间的连接权重就会增加；如果只有某个神经元单独激发，则它们之间的连接权重就应减少。

> 如果仅局限于神经网络方法范围内而言，机器学习的本质就是调节神经元之间的连接权重，即赫布法则中的神经元关联程度。

> 因为我们模型中的权重值是根据训练集中大量样本学习得出的，所以在理想情况下，其他不在训练样本集合中的图片，只要它依然是人类可辨认的阿拉伯数字图片的话，就应当同样适合这个模型，加权求和再经过阈值比较之后，网络中应该激活一个且只激活一个其数字对应的M-P神经元，这个神经元代表的数字就是图片识别的结果。

> 从这个例子里可以再提炼出一个更为普适的神经网络机器学习解释，神经网络学习方法的基本原理就是从训练集中提取出分类特征，这些特征应能同样适应独立同分布的其他未知数据，所以经已知数据学习训练后的神经网络可以对同类的未知数据有效。


### 5.3 自动机对抗自动机
> 战争的未来已可由此预见，此后的战争就是自动机对抗自动机


### 5.4 从“控制论”说起
> 维纳自己把控制论看作是一门研究机器与生物控制和通信的一般规律，以及此规律所表现出来行为的科学，是研究系统在不同环境下如何保持稳定状态的科学。


### 5.5 机械因果观和行为主义
> 给考察对象以某种刺激，观察它的反馈，通过研究反馈与刺激的关系来了解对象的特性，而不去纠结对象内部的组织结构，这就是行为主义方法。

> 如果把环境对系统的影响和作用统称为“输入”，把系统对环境的作用及其引起的环境变化称为“输出”，则给系统施加某种输入，观察它的输出，通过分析输出对输入的响应关系以了解系统的属性，而不必顾及系统内部的组织结构，这就是广义的行为主义方法。

> 连接主义学派使用的是生物仿生学的方法，通过模拟生物体的脑部组织结构去寻找智能，它关心的是承载智能的生理结构；符号主义学派使用的是逻辑推理和演算的方法，通过解析物理符号系统假说和启发式搜索原理去寻找智能，它关心的是承载智能的心理结构和逻辑结构；而行为主义学派使用“感知—动作”的研究方法，通过环境反馈和智能行为之间的因果联系去寻找智能，既不关心智能的载体和其内部的生理结构，也不关心智能的逻辑和心理结构，只关心智能的外部可观察到的行为表现。


### 6.1 概述
> 我们还应意识到能有效解决问题的机器学习是困难的，原因是没有通用的能够学习所有知识的方法，也不会有能够解决所有问题的通用知识，每一个问题都必需有针性对地通过专门的学习过程，寻找出能够解决问题的正确模型。而且，找到正确模型这件事情也是困难的，现在尚未出现一套完整的、严谨的、可一步步参照着操作的关于机器学习中应如何建模的方法论，机器学习问题大多需要处理者根据实际问题的特点和自身的经验去解决。因此，常会由于模型决策函数、参数和结构选择不当、优化策略或者算法不对、训练数据不够等原因，导致机器学习程序常不能交付令人满意的结果。


### 6.2 什么是机器学习
> 他认为只要不用程序员显式地给机器编程就能够实现某些功能便是机器学习￼。

> “如果某个系统可以从经验中改进自身的能力，那这便是学习的过程。”


### 6.3 机器学习的意义
> 说白了，机器学习是在追求让人类不用再去思考和设计的可能，让人类只需收集到足够的经验数据，让计算机能够自己去琢磨，找到模拟的办法，进而解决问题。

> 今天的机器学习还不能让计算机直接做到自己总结得出事物发展的规律，自己去设计程序来解决问题，但能够让它先去做模拟，在一定精度上做出问题可接受的近似解。

> 所以，现在机器学习如此的备受关注，它是代表了一种人类全新使用计算机的方式，很可能是未来我们应对越来越复杂问题、面对已经超过人类智力能够处理的问题时，人与计算机合作、交互的主要形式。


### 6.4 机器学习解决的问题
> 人类认知世界的两种最基本手段是“演绎”和“归纳”，有什么样的工具、手段，就能解决什么样的问题，人类能够认知的知识的范围，是由这两种手段划定的。


### 6.5 进行机器学习：实战模型训练
> 训练过程的第一步是处理如何从传感器中取得数据、怎样过滤噪声这些问题

> 从形式上说，模型就是一个可被计算的、有输出结果的方法或函数，这个函数可能是有科学含义的，也可能没有任何含义，可能用于决策，也可能用于预测。

> 从如何把问题设计出来，把现实世界中的问题，提炼成一个机器学习处理的问题开始，就需要处理者对问题本身有深刻的洞察才行。

> 数据清洗到特征筛选，到模型选择、模型优化，再到模型验证这些步骤，都伴随着好坏优劣的价值判断，这些判断不存在统一的标准和方法，均需要解决者深入具体问题，很多还需要不断尝试才能得出满意的结果。目前的机器学习理论，距离实现自动化，不再需要人类去参与的算法，还是相当的遥远。

> 为进一步深入分析和处理建立基础。我们将以样本数据作为训练集的机器学习过程称为“无监督学习”（Unsupervised Learning）。

> 这类以样例数据作为训练集的机器学习任务，被称为“监督学习”（Supervised Learning）。

> 无论训练集是由样本还是由样例构成，监督学习和非监督学习都是从历史经验之中学习，而强化学习并不主要依赖历史经验，而是一种基于环境对行为收益的评价来改进自身的模型。

> 这个学习过程需要的不是“历史数据”，而是一位“裁判”或者“老师”，用来给行为进行打分评价，并对正确的行为给予激励，对错误的行为给予惩罚。

> 保证数据是正确的部分，称为“数据清洗”（Data Cleansing），而保证数据是合适的这部分，就称为“特征选择”（Feature Selection）。

> 所谓的特征选择，是指我们应该放弃掉对结果影响轻微的特征，挑选出对结果有决定性影响的关键特征，提供给建模阶段作为模型输入使用。

> 特征选择是“数据降维”（Dimension Reduction）的一种主要方法，还有一个主要降维方法称为“特征提取”（Feature Extraction），它与特征选择的区别是：特征提取是在原有特征基础之上去创造凝练出一些新的特征出来，如果创建一个新的特征项，该特征的变化规律能够反映出原来几个特征项的共同变化，那使用这一个新特征项代替原来几个特征项就实现了降维的目的。而特征选择只是在原有特征中选取最有用的特征而已，一般并不会对特征值进行变换。

> 采用这些算法也可以实现数据降维，不过实际中要解决问题，往往必须考虑到具体模型的目标和这个领域中的先验知识，这时候就要采用自动降维算法和人工筛选特征互相配合才是比较合适的方案。

> “数据和特征决定了机器学习的上限，而模型和算法只是逼近这个上限而已。”由此可见，数据预处理，尤其是特征选择在机器学习中是占有相当重要的地位的。

> 通过各种方法，包括但不限于选定适当的策略、根据训练集中蕴含的信息优化算法、找出最相关的属性和合理模型结构等，实现让模型的输出结果与实际结果差异最小。

> 其实，所谓学习某一种机器学习算法，很大程度上就是去学习理解其损失函数的意义，然后学习如何去求解或者优化，得到满足损失函数最小值的模型结果。

> 对于目的以讲解机器学习算法为主的书籍，比如周志华老师的《机器学习》和李航老师的《统计学习方法》

> 线性支持向量机是分割样本空间的一种方法，而这句话所说的“分割样本空间的方法”，在机器学习中可表述得更为具体：我们找到了一种关于垃圾邮件的判别决策方法，一旦输入了邮件的“邮件长度”和“收件人数量”两项信息后，该方法就能无疑义地确定代表判定结果的点所在的位置和颜色，换言之，得到这封邮件的分类结果。这个决策方法，就是我们通过机器学习得到的一个能解决问题的模型。

> 在机器学习的定义中介绍到，模型是机器学习过程最终所要产出的结果，它一般会以一个可被计算的条件概率分布或决策函数形式存在。

> 那么，既然有无穷多个可能的模型，就有无穷多个可能被选择的决策函数，所有这些可能被选择到的决策函数的全集，就被称为是该模型的“假设空间”（HypothesisSpace）。选择模型，便是采取一种适当的学习策略（如例子中的支持向量机就是一种策略），再在大量数据的支持下，从假设空间中筛选出一个最佳的模型。

> 我们将“泛化能力”（Generalization Ability），就是机器学习算法对新鲜样本的适应能力，作为衡量机器学习模型的最关键的性能指标

> 误差的存在，就意味着模型输出值与实际值不相同，不相同有可能是因为模型无法表示实际数据的复杂度而造成了“偏差”（Bias）过大，或者因为模型对训练它所用的有限的数据过度敏感而造成的“方差”（Variance）过大，又或者是因为训练集中存在部分样例数据的标记值与真实结果有差别（即训练数据自身的错误），产生的“噪声”（Noise）过多，误差就是由这三个原因所导致的。要降低误差获得更好的性能，也就是要降低这三个误差的来源因素。

> 方差的含义是指给出同样数量，但内容发生了变动后的样本数据所导致的模型性能变化。方差大小的本质是描述数据扰动对模型输出结果所造成的影响。

> 如果我们要想获得较小的方差，那就应该去简化模型，缩减模型参数，降低模型的复杂度，这样才能够控制住因样本数据变化而带来的扰动幅度，越是精密复杂的模型，对输入数据的抗扰动能力就相对越差。

> 由此可见，选择最优模型复杂度的一个最基本的准则就是偏差和方差之和最小，即要同时警惕避免发生训练过少导致模型复杂度过低而欠拟合和训练过度导致模型复杂度太高而过拟合的情况发生

> 更有可能接近于最佳的模型复杂度。
在机器学习里，欠拟合是相对较容易解决的，通过增加样本、增加训练次数一般就可以解决，但是对过拟合的控制就是相对困难的问题。

> “控制模型复杂度”可以视为机器学习中除了“让模型的输出结果与实际结果差异最小”之外的第二重要的目标。


